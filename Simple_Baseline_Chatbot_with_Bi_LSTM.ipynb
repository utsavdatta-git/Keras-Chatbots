{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Baseline Chatbot with Bi-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsavdatta-git/keras_chatbots/blob/master/Simple_Baseline_Chatbot_with_Bi_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srLRCmtOO8Th",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "e8b57053-909f-492d-a864-2b5470657adf"
      },
      "source": [
        "# import all the libraries\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import json\n",
        "import re\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from gensim.parsing.preprocessing import preprocess_string,strip_punctuation, strip_numeric, strip_multiple_whitespaces, strip_non_alphanum\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, TimeDistributed\n",
        "from tensorflow.keras import preprocessing , utils\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ihbIf-FBWaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setup_google_drive():\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00LzPgFROXeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------------------------------\n",
        "# Function name: text_preprocess\n",
        "# Purpose: This fuction pre-processes an input text through a series of pre-processing pipelines.\n",
        "#          It has been implemented as a function to be used as a common function \n",
        "#          to pre-process texts at any stage of the assignment             \n",
        "# Input: a string of text\n",
        "# Output: preprocessed list of words\n",
        "#-----------------------------------------------------------------------------\n",
        "def text_preprocess(text):\n",
        "  #Strip leading and trailing spaces and De-capitalize\n",
        "  text = str(text).strip().lower()\n",
        "  #These are just common English contractions as much as possible\n",
        "  contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "  #Removing contractions\n",
        "  text_cleaned = \" \".join([contraction_dict.get(x,x) for x in text.split()])\n",
        "  \n",
        "  #Removing punctuations, removing numbers and mupliple spaces\n",
        "  CUSTOM_FILTERS = [lambda x: x, strip_punctuation, strip_numeric, strip_multiple_whitespaces,strip_non_alphanum]\n",
        "  dialogues_list_preprocessed = preprocess_string(text_cleaned, CUSTOM_FILTERS)\n",
        " \n",
        "  stemmer = PorterStemmer()\n",
        "  \n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        "  #Stemming and Lemmatizing\n",
        "  #dialogues_list_preprocessed = [lemmatizer.lemmatize(x) for x in dialogues_list_preprocessed]\n",
        "  #dialogues_list_preprocessed = [stemmer.stem(x) for x in dialogues_list_preprocessed]\n",
        "  dialogues_list_preprocessed = [x.replace(\"â€™\",\"\") for x in dialogues_list_preprocessed]\n",
        "  \n",
        "  return dialogues_list_preprocessed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLXpQS4COFT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------------------------------\n",
        "# Function name: read_conv_data\n",
        "# Purpose: This fuction reads conversation files which contain question and answer pairs             \n",
        "# Input: none\n",
        "# Output: questions, answers and list of distinct questions \n",
        "#-----------------------------------------------------------------------------\n",
        "def read_conv_data():\n",
        "  questions = []\n",
        "  answers = []\n",
        "  preprocessed_answers = []\n",
        "  df = pd.read_csv(\"/content/drive/My Drive/Cousera files/qna_chitchat_professional.tsv\", sep=\"\\t\")\n",
        "  for index, row in df.iterrows():\n",
        "      questions.append(' '.join(text_preprocess(row[0])))\n",
        "      preprocessed_answers.append(' '.join(text_preprocess(row[1])))\n",
        "      answers.append(row[1])  \n",
        "  distinct_answers = set(answers)  \n",
        "  return questions, answers, preprocessed_answers, distinct_answers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7bsN0XZPGBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------------------------------\n",
        "# Function name: create_question_answer_batches\n",
        "# Purpose: This function modifies question and answers to make them fit to input into neural network             \n",
        "# Input: none\n",
        "# Output: padded numeric list of questions and list of answers-to-numeric value mapping \n",
        "#-----------------------------------------------------------------------------\n",
        "def create_question_answer_batches():\n",
        "  questions, answers, preprocessed_answers, distinct_answers = read_conv_data()\n",
        "  tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "  tokenizer.fit_on_texts(questions+preprocessed_answers)\n",
        "  sequences_question = tokenizer.texts_to_sequences(questions)\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "  # input_data\n",
        "  max_question_len = max([len(x.split()) for x in questions])\n",
        "  data_questions = pad_sequences(sequences_question, maxlen=max_question_len,padding='post')\n",
        "  max_length = data_questions.shape[1]\n",
        "  # output_data\n",
        "  answers_dict={}\n",
        "  num_to_answers_dict={}\n",
        "  labels = []\n",
        "  for num, ans in enumerate(distinct_answers):\n",
        "    answers_dict[ans] = num\n",
        "    num_to_answers_dict[num] = ans\n",
        "  for answer in answers:\n",
        "    labels.append(answers_dict[answer])\n",
        "  labels_one_hot = utils.to_categorical(labels)  \n",
        "  return word_index, data_questions, answers_dict, num_to_answers_dict, labels_one_hot, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPx3UPdkV3e4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------------------------------\n",
        "# Function name: create_glove_embed_index\n",
        "# Purpose: This function creates a word to vector model from a pre-trained Glove model             \n",
        "# Input: none\n",
        "# Output: glove word embedding dictionary \n",
        "#-----------------------------------------------------------------------------\n",
        "def create_glove_embed_index():\n",
        "  embeddings_index = {}\n",
        "  f = open('/content/drive/My Drive/Cousera files/glove.6B.100d.txt')\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "  return embeddings_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joSTu5D2WNpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------------------------------\n",
        "# Function name: create_embedding_matrix\n",
        "# Purpose: This function creates a word index to vector matrix             \n",
        "# Input: vocab_size, embedding_dim, word_index\n",
        "# Output: word embedding matrix \n",
        "#-----------------------------------------------------------------------------\n",
        "def create_embedding_matrix(vocab_size, embedding_dim, word_index):\n",
        "  embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "  embeddings_index = create_glove_embed_index()\n",
        "  for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ2hPWE2Z5z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------------------------------\n",
        "# Function name: create_model\n",
        "# Purpose: create a keras model to train the chatbot             \n",
        "# Input: vocab_size, LSTM_nodes, embedding_dim, word_index\n",
        "# Output: the model\n",
        "#-----------------------------------------------------------------------------\n",
        "def create_model(vocab_size, LSTM_nodes, embedding_dim, word_index):\n",
        "  embedding_matrix = create_embedding_matrix(vocab_size, embedding_dim, word_index)\n",
        "  model = tf.keras.Sequential([\n",
        "                              tf.keras.layers.Embedding(input_dim = vocab_size, \n",
        "                                output_dim = embedding_dim,\n",
        "                                weights = [embedding_matrix],\n",
        "                                trainable = False),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_nodes)),\n",
        "                              tf.keras.layers.Dense(labels_one_hot.shape[1], activation=\"softmax\")\n",
        "  ])\n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myWdLX4ckZZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------------------------------\n",
        "# Function name: start_chat\n",
        "# Purpose: start the chat using the trained model             \n",
        "# Input: tokenizer, max_length, model, num_to_answers_dict\n",
        "# Output: none\n",
        "#-----------------------------------------------------------------------------\n",
        "def start_chat(tokenizer, max_length, model, num_to_answers_dict):\n",
        "  stop_condition = False\n",
        "  while not stop_condition :\n",
        "    user_conv = input(\"User: \")\n",
        "    if user_conv == \"Exit\":\n",
        "      break\n",
        "    sequenced_new_data = tokenizer.texts_to_sequences([user_conv])\n",
        "    padded_new_data = pad_sequences(sequenced_new_data, padding=\"post\", maxlen=max_length)\n",
        "    pred = model.predict_classes(padded_new_data)\n",
        "    response = num_to_answers_dict[pred[0]]\n",
        "    print(\"Bot: \"+response)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbwW8aiYjBSZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7652723-461c-42e1-9748-3366b8d27919"
      },
      "source": [
        "# Put it all together\n",
        "# Setup\n",
        "setup_google_drive()\n",
        "word_index, data_questions, answers_dict, num_to_answers_dict, labels_one_hot, tokenizer = create_question_answer_batches()\n",
        "# Define constants\n",
        "vocab_size = len(word_index)+1\n",
        "embedding_dim = 100\n",
        "LSTM_nodes = 64\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "conv_model = create_model(vocab_size, LSTM_nodes, embedding_dim, word_index)\n",
        "# Start training\n",
        "conv_model.fit(data_questions, labels_one_hot, batch_size=batch_size, epochs=epochs) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 2625 unique tokens.\n",
            "Found 400000 word vectors.\n",
            "Train on 9796 samples\n",
            "Epoch 1/100\n",
            "9796/9796 [==============================] - 12s 1ms/sample - loss: 3.1041 - acc: 0.3112\n",
            "Epoch 2/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 1.7998 - acc: 0.5731\n",
            "Epoch 3/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 1.3034 - acc: 0.6874\n",
            "Epoch 4/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 1.0131 - acc: 0.7588\n",
            "Epoch 5/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.8321 - acc: 0.8006\n",
            "Epoch 6/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.6991 - acc: 0.8323\n",
            "Epoch 7/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.5959 - acc: 0.8559\n",
            "Epoch 8/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.5116 - acc: 0.8757\n",
            "Epoch 9/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.4453 - acc: 0.8965\n",
            "Epoch 10/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.3873 - acc: 0.9102\n",
            "Epoch 11/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.3366 - acc: 0.9231\n",
            "Epoch 12/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.2954 - acc: 0.9312\n",
            "Epoch 13/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.2595 - acc: 0.9417\n",
            "Epoch 14/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.2318 - acc: 0.9473\n",
            "Epoch 15/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.2038 - acc: 0.9568\n",
            "Epoch 16/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.1804 - acc: 0.9593\n",
            "Epoch 17/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.1593 - acc: 0.9664\n",
            "Epoch 18/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.1381 - acc: 0.9717\n",
            "Epoch 19/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.1227 - acc: 0.9754\n",
            "Epoch 20/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.1062 - acc: 0.9796\n",
            "Epoch 21/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0990 - acc: 0.9811\n",
            "Epoch 22/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0883 - acc: 0.9840\n",
            "Epoch 23/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0766 - acc: 0.9858\n",
            "Epoch 24/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0668 - acc: 0.9883\n",
            "Epoch 25/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0627 - acc: 0.9888\n",
            "Epoch 26/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0556 - acc: 0.9900\n",
            "Epoch 27/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0521 - acc: 0.9905\n",
            "Epoch 28/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0457 - acc: 0.9915\n",
            "Epoch 29/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0446 - acc: 0.9914\n",
            "Epoch 30/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0390 - acc: 0.9927\n",
            "Epoch 31/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0355 - acc: 0.9938\n",
            "Epoch 32/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0331 - acc: 0.9934\n",
            "Epoch 33/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0340 - acc: 0.9918\n",
            "Epoch 34/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0289 - acc: 0.9944\n",
            "Epoch 35/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0296 - acc: 0.9943\n",
            "Epoch 36/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0292 - acc: 0.9935\n",
            "Epoch 37/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0248 - acc: 0.9949\n",
            "Epoch 38/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0247 - acc: 0.9947\n",
            "Epoch 39/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0252 - acc: 0.9944\n",
            "Epoch 40/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0241 - acc: 0.9942\n",
            "Epoch 41/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0268 - acc: 0.9936\n",
            "Epoch 42/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0243 - acc: 0.9944\n",
            "Epoch 43/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0240 - acc: 0.9944\n",
            "Epoch 44/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0228 - acc: 0.9950\n",
            "Epoch 45/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0211 - acc: 0.9946\n",
            "Epoch 46/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0217 - acc: 0.9940\n",
            "Epoch 47/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0222 - acc: 0.9944\n",
            "Epoch 48/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0210 - acc: 0.9943\n",
            "Epoch 49/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0194 - acc: 0.9942\n",
            "Epoch 50/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0189 - acc: 0.9949\n",
            "Epoch 51/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0221 - acc: 0.9944\n",
            "Epoch 52/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0202 - acc: 0.9947\n",
            "Epoch 53/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0213 - acc: 0.9944\n",
            "Epoch 54/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0178 - acc: 0.9951\n",
            "Epoch 55/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0193 - acc: 0.9953\n",
            "Epoch 56/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0185 - acc: 0.9953\n",
            "Epoch 57/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0169 - acc: 0.9956\n",
            "Epoch 58/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0168 - acc: 0.9956\n",
            "Epoch 59/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0180 - acc: 0.9952\n",
            "Epoch 60/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0183 - acc: 0.9943\n",
            "Epoch 61/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0168 - acc: 0.9953\n",
            "Epoch 62/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0185 - acc: 0.9952\n",
            "Epoch 63/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0181 - acc: 0.9946\n",
            "Epoch 64/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0168 - acc: 0.9956\n",
            "Epoch 65/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0175 - acc: 0.9945\n",
            "Epoch 66/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0188 - acc: 0.9945\n",
            "Epoch 67/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0169 - acc: 0.9952\n",
            "Epoch 68/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0176 - acc: 0.9951\n",
            "Epoch 69/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0162 - acc: 0.9952\n",
            "Epoch 70/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0179 - acc: 0.9951\n",
            "Epoch 71/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0174 - acc: 0.9954\n",
            "Epoch 72/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0168 - acc: 0.9952\n",
            "Epoch 73/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0161 - acc: 0.9953\n",
            "Epoch 74/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0170 - acc: 0.9951\n",
            "Epoch 75/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0174 - acc: 0.9957\n",
            "Epoch 76/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0163 - acc: 0.9957\n",
            "Epoch 77/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0157 - acc: 0.9957\n",
            "Epoch 78/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0162 - acc: 0.9954\n",
            "Epoch 79/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0166 - acc: 0.9950\n",
            "Epoch 80/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0172 - acc: 0.9955\n",
            "Epoch 81/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0165 - acc: 0.9958\n",
            "Epoch 82/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0156 - acc: 0.9956\n",
            "Epoch 83/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0156 - acc: 0.9956\n",
            "Epoch 84/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0158 - acc: 0.9953\n",
            "Epoch 85/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0166 - acc: 0.9951\n",
            "Epoch 86/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0163 - acc: 0.9958\n",
            "Epoch 87/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0163 - acc: 0.9952\n",
            "Epoch 88/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0160 - acc: 0.9951\n",
            "Epoch 89/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0158 - acc: 0.9955\n",
            "Epoch 90/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0156 - acc: 0.9952\n",
            "Epoch 91/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0155 - acc: 0.9956\n",
            "Epoch 92/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0157 - acc: 0.9954\n",
            "Epoch 93/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0158 - acc: 0.9959\n",
            "Epoch 94/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0152 - acc: 0.9959\n",
            "Epoch 95/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0146 - acc: 0.9962\n",
            "Epoch 96/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0151 - acc: 0.9958\n",
            "Epoch 97/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0156 - acc: 0.9950\n",
            "Epoch 98/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0150 - acc: 0.9957\n",
            "Epoch 99/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0155 - acc: 0.9956\n",
            "Epoch 100/100\n",
            "9796/9796 [==============================] - 11s 1ms/sample - loss: 0.0158 - acc: 0.9957\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb148e14198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66iii5q4bfYN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "54c5867d-928a-4935-f6ef-098291c6e2e8"
      },
      "source": [
        "start_chat(tokenizer, data_questions.shape[1], conv_model, num_to_answers_dict)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User: Hello dear! How are you?\n",
            "Bot: I'm digital. In other words, I'm not human.\n",
            "User: I know but you are really nice\n",
            "Bot: Noted.\n",
            "User: what are you upto?\n",
            "Bot: I'm digital. In other words, I'm not human.\n",
            "User: Do you eat?\n",
            "Bot: I don't have a body.\n",
            "User: Do you have any problems?\n",
            "Bot: I don't have family.\n",
            "User: I like you very very much\n",
            "Bot: Thanks.\n",
            "User: Do you like me?\n",
            "Bot: I do likeÂ you.\n",
            "User: Is Australia a great country?\n",
            "Bot: Ok.\n",
            "User: are all robots cruel?\n",
            "Bot: Not at all.\n",
            "User: what kind of chatbot are you?\n",
            "Bot: I'm digital. In other words, I'm not human.\n",
            "User: Is life good?\n",
            "Bot: I'm happy to hear that.\n",
            "User: that was a question\n",
            "Bot: Excellent.\n",
            "User: Exit\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}